"""A module that provides a function to complete a prompt with OpenAI's model."""
import random as rand

import openai

from config import load_config


openai.api_key = load_config().openai.token

MESSAGES = {
    "nobother": [
        "Сегодня я не в настроении болтать. Пожалуйста, не раздражайте меня!",
        "Сегодня я не хочу ничего рассказывать. Не трогайте меня!",
        "Сегодня я ничему не рад. Никого не слушаю!",
        "Скорость, скорость - это моя философия! Никому ничему!",
        "Я - занят своими думами! Нельзя, чтобы кто-нибудь мешал мне!",
        "Я - свободный! Когда не хочу отвечать - не отвечаю!",
    ],
    "noinput": [
        "Ты должен что-то сказать, иначе мы не сможем продолжить разговор.",
        "Ты должен что-то ввести, иначе разговор не случится.",
        "Ты должен что-то сказать, иначе разговору нельзя будет уделить внимания.",
        "Ты должен что-то сказать, чтобы развивался разговор.",
        "Ты должен хоть что-нибудь сказать.",
    ],
}


def get_message(messages_key: str) -> str:
    """Returns a random message from the `MESSAGES` dictionary.

    Args:
        messages_key: The key of the MESSAGES dictionary (gets a messages list).

    Returns:
        str: A random message from the MESSAGES dictionary corresponding to
             the given key.

    Raises:
        KeyError: If the given key is not present in the MESSAGES dictionary.
    """
    msg_index = rand.randint(0, len(MESSAGES[messages_key]) - 1)
    return MESSAGES[messages_key][msg_index]


async def complete(
    prompt: str,
    stop: str,
    model: str,
    temperature: str,
    max_tokens: int,
    frequency_penalty: float,
    presence_penalty: float,
) -> str:
    """Completes the given prompt using OpenAI's language model.

    Args:
        prompt: The input text prompt to generate the completion.
        stop: Sequence where the API will stop generating further tokens.
        model: The name of the model to use for generating the completion.
        temperature: Controls the randomness of the generated completions.
        max_tokens: The maximum number of tokens to generate in the completion.
        frequency_penalty: Controls how much to penalize new tokens
            based on their frequency in the generated text so far.
        presence_penalty: Controls how much to penalize new tokens based on
            whether they appeared in the prompt or not.

    Returns:
        str: The completed text generated by the model.
    """
    if not prompt:
        return get_message("noinput")
    try:
        response = await openai.Completion.acreate(
            model=model,
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            stop=stop,
        )
    except Exception as error:
        print(error)
        return get_message("nobother")
    return response["choices"][0]["text"].strip()
